\documentclass{scrartcl}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{lightcyan}{HTML}{E0FFFF}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref}
\usepackage{graphicx}


\begin{document}

    % kudos to: https://github.com/ghammock/LaTeX_Listings_JavaScript_ES6!

    \lstdefinelanguage{JavaScript}{
        morekeywords=[1]{break, continue, delete, else, for, function, if, in,
            new, return, this, typeof, var, void, while, with},
        % Literals, primitive types, and reference types.
        morekeywords=[2]{false, null, true, boolean, number, undefined,
            Array, Boolean, Date, Math, Number, String, Object},
        % Built-ins.
        morekeywords=[3]{eval, parseInt, parseFloat, escape, unescape},
        sensitive,
        morecomment=[s]{/*}{*/},
        morecomment=[l]//,
        morecomment=[s]{/**}{*/}, % JavaDoc style comments
        morestring=[b]',
        morestring=[b]"
    }[keywords, comments, strings]

    \lstalias[]{ES6}[ECMAScript2015]{JavaScript}

    \lstdefinelanguage[ECMAScript2015]{JavaScript}[]{JavaScript}{
        morekeywords=[1]{await, async, case, catch, class, const, default, do,
            enum, export, extends, finally, from, implements, import, instanceof,
            let, static, super, switch, throw, try},
        morestring=[b]` % Interpolation strings.
    }

    % Requires package: color.
    \definecolor{mediumgray}{rgb}{0.3, 0.4, 0.4}
    \definecolor{mediumblue}{rgb}{0.0, 0.0, 0.8}
    \definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
    \definecolor{darkviolet}{rgb}{0.58, 0.0, 0.83}
    \definecolor{royalblue}{rgb}{0.25, 0.41, 0.88}
    \definecolor{crimson}{rgb}{0.86, 0.8, 0.24}
    \definecolor{lightgreen}{HTML}{F0FFF0}

    \lstdefinestyle{JSES6Base}{
        backgroundcolor=\color{lightgreen},
        basicstyle=\ttfamily,
        breakatwhitespace=false,
        breaklines=false,
        captionpos=b,
        columns=fullflexible,
        commentstyle=\color{mediumgray}\upshape,
        emph={},
        emphstyle=\color{crimson},
        extendedchars=true,  % requires inputenc
        fontadjust=true,
        frame=single,
        identifierstyle=\color{black},
        keepspaces=true,
        keywordstyle=\color{mediumblue},
        keywordstyle={[2]\color{darkviolet}},
        keywordstyle={[3]\color{royalblue}},
        numbers=left,
        numbersep=5pt,
        numberstyle=\tiny\color{black},
        rulecolor=\color{black},
        showlines=true,
        showspaces=false,
        showstringspaces=false,
        showtabs=false,
        stringstyle=\color{forestgreen},
        tabsize=2,
        title=\lstname,
        upquote=true  % requires textcomp
    }

    \lstdefinestyle{JavaScript}{
        language=JavaScript,
        style=JSES6Base
    }

    \lstdefinestyle{ES6}{
        language=ES6,
        style=JSES6Base
    }

    \lstdefinestyle{myBash}{
        language=bash,
        numbers=left,
        stepnumber=1,
        numbersep=10pt,
        tabsize=4,
        showspaces=false,
        showstringspaces=false,
        basicstyle=\small,
        backgroundcolor=\color{lightcyan}
    }


\section{The Node Binary}
\subsection{Common Command Line Arguments}

    \begin{lstlisting}[style=myBash]
    # only check syntax
    node --check app.js
    node -c app.js

    # evaluate (but don't print)
    node --eval "1+1"
    node -e "console.log(1+1)"
    2
    node -e "console.log(1+1); 0"
    2

    # evaluate and print
    node -e "console.log(1+1)"
    2
    undefined

    node -p "console.log(1+1); 0"
    2
    0
    \end{lstlisting}


\subsection{Module availability}

    All Node core modules can be accessed by their namespaces within the code evaluation context - no require required:

    \begin{lstlisting}[style=myBash]
    node -p "fs.readdirSync('.').filter((f) => /.js$/.test(f))"
    []
    \end{lstlisting}

\subsection{Preloading files}

    \begin{lstlisting}[style=ES6]
    // preload.js
    console.log('preload.js: this is preloaded')

    // app.js
    console.log('app.js: this is the main file')
    \end{lstlisting}

    \begin{lstlisting}[style=myBash]
    // CommonJS
    node -r ./preload.js app.js
    node --require ./preload.js app.js

    // ES modules
    node --loader ./preload.js app.js
    \end{lstlisting}


\subsection{Stack trace limit}

    By default, only the first 10 stack frames are shown, which can lead to the root cause of the error not being shown.

    In this case, modify the V8 option --stack-trace-limit:

    \begin{lstlisting}[style=myBash]
    node --stack-trace-limit=20 file.js
    \end{lstlisting}

\section{Debugging and Diagnostics}

    Start node in debugging mode:

    \begin{lstlisting}[style=myBash]
        node --inspect file.js # runs immediately
        node --inspect-brk file.js # breakpoint at start of program
    \end{lstlisting}

\section{Core JavaScript Concepts}
\subsection{Types}

    Everything besides the following primitive types is an object - functions and arrays, too, are objects.

\subsubsection{Primitive Types}

    \begin{lstlisting}[style=ES6]
        // The null primitive is typically used to describe
        // the absence of an object...
        // Null
        null

        // Undefined
        // ... whereas undefined is the absence
        // of a defined value.
        // Any variable initialized without a value will be undefined.
        // Any expression that attempts access of a non-existent
        property on an object will result in undefined.
        // A function without a return statement will return undefined.
        // undefined

        // Number
        // The Number type is double-precision floating-point format.
        // It allows both integers and decimals but
        // has an integer range of $-2^53-1$ to $2^53-1$.
        1, 1.5, -1e4, NaN

        // BigInt
        // The BigInt type has no upper/lower limit on integers.
        1n, 9007199254740993n

        // String
        'str', "str", `str ${var}`

        // Boolean
        true, false

        // Symbol
        // Symbols can be used as unique identifier keys in objects.
        //The Symbol.for method creates/gets a global symbol.
        Symbol('description'), Symbol.for('namespace')...

    \end{lstlisting}

\subsubsection{Object}

    An object is a set of key value pairs, where values can be any primitive type or an object (including functions, since functions are objects). Object keys are called properties.

    All JavaScript objects have prototypes.
    A prototype is an implicit reference to another object that is queried in property lookups.
    If an object doesn't have a particular property, the object's prototype is checked for that property. and so on. This is how inheritance in JavaScript works.

\subsubsection{Functions}

    \begin{lstlisting}[style=ES6]
        // this refers to the object on which the function was called,
        // not the object the function was assigned to
        const obj = { id: 999, fn: function () { console.log(this.id) } }
        const obj2 = { id: 2, fn: obj.fn }
        obj2.fn() // prints 2
        obj.fn() // prints 999

        // Functions have a call method that can be used
        // to set their this context. See
        /**
        * Calls a method of an object,
        * substituting another object for the current object.
        * @param thisArg The object to be used as the current object.
        * @param argArray A list of arguments to be passed to the method.
        */
        call(this: Function, thisArg: any, ...argArray: any[]): any;

        function fn() { console.log(this.id) }
        const obj = { id: 999 }
        const obj2 = { id: 2 }
        fn.call(obj2) // prints 2
        fn.call(obj) // prints 999

        /*
        *Lambda functions do not have their own this context.
        * Wwhen this is *referenced inside a function,
        * it refers to the this of the
        *nearest parent non-lambda function.
        */
        function fn() {
            return (offset) => {
                console.log(this.id + offset)
            }
        }
        const obj = { id: 999 }
        const offsetter = fn.call(thisArg = obj)
        console.log(typeof(offsetter)); // function
        offsetter(1) // 1000
    \end{lstlisting}


    Lambda functions do not have a prototype.



\subsection{Prototypal Inheritance}
\subsubsection{Prototypal inheritance - functional}
    Create a prototype chain:
\begin{lstlisting}[style=ES6]
    const wolf = {
        howl: function () { console.log(this.name + ': awoooooooo') }
    }

    const dog = Object.create(wolf, {
        woof: { value: function() { console.log(this.name + ': woof') } }
    })

    const rufus = Object.create(dog, {
        name: {value: 'Rufus the dog'}
    })

    rufus.woof() // prints "Rufus the dog: woof"
    rufus.howl() // prints "Rufus the dog: awoooooooo"
\end{lstlisting}


A Property Descriptor is a JavaScript object that describes the characteristics of the properties on another object.

\begin{lstlisting}[style=ES6]
   const propdesc = Object.getOwnPropertyDescriptors(rufus);
   console.log(propdesc);

   // output
   {
       name: {
           value: 'Rufus the dog',
           writable: false,
           enumerable: false,
           configurable: false
       }
   }


   const name = Object.getOwnPropertyDescriptor(rufus, "name");
   console.log(name);

   // output

   {
       value: 'Rufus the dog',
       writable: false,
       enumerable: false,
       configurable: false
   }

\end{lstlisting}

\subsubsection{Prototypal inheritance - using constructor}

Creating an object with a specific prototype object can also be achieved by calling a function with the new keyword.

The constructor approach to creating a prototype chain is to define properties on a function's prototype object and then call that function with new.

Define how to create the parent object:

\begin{lstlisting}[style=ES6]
    function Wolf (name) {
        this.name = name
    }

    Wolf.prototype.howl = function () {
        console.log(this.name + ': awoooooooo')
    }
\end{lstlisting}

Define a function to set up the inheritance chain:

\begin{lstlisting}[style=ES6]
    function inherit (proto) {
        function ChainLink(){}
        ChainLink.prototype = proto
        return new ChainLink()
    }
\end{lstlisting}

Define how to obtain a child object:

\begin{lstlisting}[style=ES6]
    function Dog (name) {
        Wolf.call(this, name + ' the dog')
    }

    Dog.prototype = inherit(Wolf.prototype)

    Dog.prototype.woof = function () {
        console.log(this.name + ': woof')
    }

\end{lstlisting}

Create a child object():

\begin{lstlisting}[style=ES6]
    const rufus = new Dog('Rufus')

    rufus.woof() // prints "Rufus the dog: woof"
    rufus.howl() // prints "Rufus the dog: awoooooooo"
\end{lstlisting}

 In JavaScript runtimes that support EcmaScript 5+ the Object.create function could be used to the same effect:


\begin{lstlisting}[style=ES6]

    function Dog (name) {
        Wolf.call(this, name + ' the dog')
    }

    Dog.prototype = Object.create(Wolf.prototype)

    Dog.prototype.woof = function () {
        console.log(this.name + ': woof')
    }
\end{lstlisting}

Node.js has a utility function: util.inherits that is often used in code bases using constructor functions.

\begin{lstlisting}[style=ES6]

    const util = require('util')

    function Dog (name) {
        Wolf.call(this, name + ' the dog')
    }

    Dog.prototype.woof = function () {
        console.log(this.name + ': woof')
    }

    // sets the prototype of Dog.prototype to Wolf.prototype
    util.inherits(Dog, Wolf)

\end{lstlisting}

In contemporary Node.js, util.inherits uses the EcmaScript 2015 (ES6) method Object.setPrototypeOf under the hood.

\begin{lstlisting}[style=ES6]
    Object.setPrototypeOf(Dog.prototype, Wolf.prototype)
\end{lstlisting}


\subsubsection{Prototypal inheritance - class-based}

The class keyword is syntactic sugar that actually creates a function, new(), that is to be used as a constructor.
Internally, it creates prototype chains.

Usage:

\begin{lstlisting}[style=ES6]
    class Wolf {
        constructor (name) {
            this.name = name
        }
        howl () { console.log(this.name + ': awoooooooo') }
    }

    class Dog extends Wolf {
        constructor(name) {
            super(name + ' the dog')
        }
        woof () { console.log(this.name + ': woof') }
    }

    const rufus = new Dog('Rufus')
\end{lstlisting}

\subsection{Inheritance provided by closures}

Below, the spread operator is used. The spread operator splits arrays into their members, and object into properties.
When spreading objects, the properties are added as key-value pairs.

\begin{lstlisting}[style=ES6]
    function wolf (name) {
        const howl = () => {
            console.log(name + ': awoooooooo')
        }
        return { howl: howl }
    }

    function dog (name) {
        name = name + ' the dog'
        const woof = () => { console.log(name + ': woof') }
        return {
            ...wolf(name),
            woof: woof
        }
    }
    const rufus = dog('Rufus')

    console.log(rufus)
    // { howl: [Function: howl], woof: [Function: woof] }
    rufus.woof()
    // "Rufus the dog: woof"
    rufus.howl()
    // "Rufus the dog: awoooooooo"
\end{lstlisting}

\section{Packages and Dependencies}

\subsection{Specifying a SemVer range}

\begin{itemize}
    \item Prefix the version with a caret (\lstinline|^|) to include everything that does not increment the first non-zero portion of semver. Example: \lstinline|^8.14.1| is the same as \lstinline|8.x.x|.

    Note:  caret behavior is different for 0.x versions, for which it will only match patch versions.
    \item Use the tilde symbol to include everything greater than a particular version in the same minor range. Example: \lstinline|~2.2.0| matches \lstinline|2.2.0| and \lstinline|2.2.1| (highest existing minor version).
    \item Specify a range of versions. Example: \lstinline|>2.1| matches \lstinline|2.2.0|,
    \lstinline|2.2.1|, ... Note: There must be spaces on either side of hyphens.
    \item Use \lstinline{||} to combine multiple sets of versions. Example: \lstinline{^2 <2.2 || > 2.3}.

\end{itemize}

\section{Module System}

\subsection{Detecting Main Module in CJS}

In some situations we may want a module to be able to operate both as a program and as a module that can be loaded into other modules.

When a file is the entry point of a program, it's the main module. We can detect whether a particular file is the main module.

\begin{lstlisting}[style=ES6]
    // imports

    if (require.main === module) {
       // ...
    } else {
        const myfunc = (str) => {
            // ...
        }
        module.exports = myfunc
    }
\end{lstlisting}

The "start" script in the package.json file executes \lstinline|node index.js|. When a file is called with node that file is the \textit{entry point of a program}.

\begin{lstlisting}[style=myBash]
    npm start

    // or:
    node index.js

    // app starts
\end{lstlisting}

If it is loaded as a module, it will export function myfunc:

\begin{lstlisting}[style=myBash]
   $node -p 'require("./index.js")'
   [Function: myfunc]

   $ node -p 'require("./index.js")("test")'
   TSET
\end{lstlisting}

\subsection{Converting a Local CJS File to a Local ESM File}

When using ECMA Script modules in a CJS application, module files need to have the .mjs extension.

\begin{lstlisting}[style=myBash]
    node -e "fs.renameSync('./format.js', './format.mjs');"
    node -p "fs.readdirSync('.').join('\t');"
\end{lstlisting}

\lstinline|require()| cannot be used with ESM modules.

Instead, we need to change it to a dynamic \lstinline|import()| which is available in all CommonJS modules.

This is due to CJS loading modules synchronously, while ESM loads modules asynchronously.
As a consequence, ESM can import CJS, but CJS cannot require ESM since that would break the synchronous constraint.

\subsubsection{Aside: Static and Dynamic Imports}

Assume we have a file utils.mjs

\begin{lstlisting}[style=ES6]
    // Default export
    export default () => {
        console.log('Hi from the default export!');
    };

    // Named export `doStuff`
    export const doStuff = () => {
        console.log('Doing stuff');
    };
\end{lstlisting}

This is a static import:

\begin{lstlisting}[style=ES6]
    import * as module from './utils.mjs';
\end{lstlisting}

Whereas this is a dynamic import:

\begin{lstlisting}[style=ES6]

    // returns a Promise
    import('./utils.mjs')
    .then((module) => {
        module.default();
        // logs 'Hi from the default export!'
        module.doStuff();
        // logs 'Doing stuff'
    });

    // or
    (async () => {
        const moduleSpecifier = './utils.mjs';
        const module = await import(moduleSpecifier)
        module.default();
        // logs 'Hi from the default export!'
        module.doStuff();
        // logs 'Doing stuff'
    })();
\end{lstlisting}

\subsection{Converting a CJS Package to an ESM Package}

\subsubsection{Specify module type}

We can opt-in to ESM-by-default by adding a type field to the package.json and setting it to "module". Our package.json should look as follows:


\begin{lstlisting}[style=ES6]
    {
        "name": "my-package",
        "version": "1.0.0",
        "main": "index.js",
        "type": "module",
        //...
    }

\end{lstlisting}

\subsubsection{Exports in ESM}

Whereas in CJS, we assigned a function to module.exports:

\begin{lstlisting}[style=ES6]
      module.exports = myfunc
\end{lstlisting}

in ESM we use the \lstinline|export default| keyword and follow with a function expression to set a function as the main export:

\begin{lstlisting}[style=ES6]
    export default (str) => {
        return somefunc(str);
    }
\end{lstlisting}

 The default exported function is synchronous again, as it should be.

 Note: ESM exports must be statically analyzable; and this means they can't be conditionally declared. The export keyword only works at the top level.

EcmaScript Modules were primarily specified for browsers, implying that there is no concept of a main module in the spec (since modules are initially loaded via HTML, which could allow for multiple script tags).

We can however infer that a module is the first module executed by Node by comparing process.argv[1] (which contains the execution path of the entry file) with import.meta.url.

\begin{lstlisting}[style=ES6]
    const isMain = process.argv[1] &&
    await realpath(fileURLToPath(import.meta.url)) ===
    await realpath(process.argv[1])
\end{lstlisting}

One compelling feature of modern ESM is \textit{Top-Level Await (TLA)}. Since all ESM modules load asynchronously it's possible to perform related asynchronous operations as part of a module's initialization.

TLA allows the use of the await keyword in an ESM modules scope at the top level, in addition to the standard usage within async functions.

With a dynamic import, if we want to use an imported module as default export, we have to reassign the default property to it. That's because dynamic imports return a promise which resolves to an object.
If there's a default export in a module, the default property of that object will be set to it.

\begin{lstlisting}[style=ES6]
    if (isMain) {
        const { default: pino } = await import('pino')
        const logger = pino()
       //
    }

    export default (str) => {
        return format.upper(str).split('').reverse().join('')
    }
\end{lstlisting}

\subsubsection{Importing modules}

With static imports, different import possibilities exist:

\begin{itemize}
    \item Implicitly import a module's default export
    \begin{lstlisting}[style=ES6]
        // the default export of the url module is assigned
        //  to the url reference.
        import url from 'url'
    \end{lstlisting}

    \item Import a specific named export from a module
    \begin{lstlisting}[style=ES6]
        import { realpath } from 'fs/promises'
    \end{lstlisting}

    \item If there are no default exports, just individual ones, the following syntax is used:
    \begin{lstlisting}[style=ES6]
        import * as format from './format.js'
    \end{lstlisting}

    If a module \textit{does} have a default export and that same syntax - \lstinline|import * as| - is used to load it, the resulting object will have a \lstinline|default| property holding the default export.

    Note: ESM does not support loading modules without a  full file extension.
\end{itemize}

\subsection{Resolving a Module Path in CJS}
The require function has a method called r\lstinline|equire.resolve|. This can be used to determine the absolute path for any required module.

Example:

\begin{lstlisting}[style=ES6]
    # package resolution
    # no path given: looks into node-modules
    require('pino')  => /home/key/code/[...]/app/node_modules/pino/pino.js
    require('standard')  => /[...]/app/node_modules/standard/index.js

    # directory resolution
    # resolves to index.js!
    require('.') 	=> /home/key/code/[...]/app/index.js
    require('../app') => /home/key/code/[...]/app/index.js

    # file resolution
    # path given: resolves to local file
    # both with and without extension work
    require('./format')  => /home/key/code/[...]/app/format.js
    require('./format.js')  => /home/key/code/[...]/app/format.js

    # core APIs resolution
    require('fs') 	   => fs
    require('util') 	   => util

\end{lstlisting}

\subsection{Resolving a Module Path in ESM}

Currently there is experimental support for an \lstinline|import.meta.resolve| function which returns a promise that resolves to the relevant file:// URL for a given valid input. Since this is experimental, and behind the \lstinline|--experimental-import-meta-resolve| flag, we'll discuss an alternative approach to module resolution inside an EcmaScript Module.

We can use the ecosystem \lstinline|import-meta-resolve| module to get the best results for now.

import { resolve } from 'import-meta-resolve'

console.log(
`import 'pino'`,
'=>',
await resolve('pino', import.meta.url)
)

console.log(
`import 'tap'`,
'=>',
await resolve('tap', import.meta.url)
)





\begin{lstlisting}[style=ES6]
    import { resolve } from 'import-meta-resolve'

    console.log(
    `import 'pino'`,
    '=>',
    await resolve('pino', import.meta.url)
    )

    // If a package's package.json exports field defines
    // an ESM entry point, the require.resolve function will still
    // resolve to the CJS entry point because require is a CJS API.
    // import-meta-resolve has a workaround
    console.log(
    `import 'tap'`,
    '=>',
    await resolve('tap', import.meta.url)
    )

    // resolved to [...]/tap/dist/esm/index.js


\end{lstlisting}

\section{ Asynchronous Control Flow }

\subsection{Callbacks}

Here, the readFile function schedules a task, which is to read the given file. When the file has been read, the readFile function will call the function provided as the second argument.

The second argument to readFile is a function that has two parameters, err and contents. This function will be called when readFile has completed its task. If there was an error, then the first argument passed to the function will be an error object representing that error, otherwise it will be null. If the readFile function is successful, the first argument (err) will be null and the second argument (contents) will be the contents of the file.

\begin{lstlisting}[style=ES6]
    const { readFile } = require('fs')

    // __filename holds the path of the file currently being executed
    readFile(__filename, (err, contents) => {
        if (err) {
            console.error(err)
            return
        }
        console.log(contents.toString())
    })
\end{lstlisting}

This yields a way to achieve parallel execution in Node.js:

\begin{lstlisting}[style=ES6]
    const { readFile } = require('fs')
    const [ bigFile, mediumFile, smallFile ] =
        Array.from(Array(3)).fill(__filename)

    const print = (err, contents) => {
        if (err) {
            console.error(err)
            return
        }
        console.log(contents.toString())
    }
    readFile(bigFile, print)
    readFile(mediumFile, print)
    readFile(smallFile, print)
\end{lstlisting}

Here the smallest file will be printed first, even though it's scheduled to be read last.

If instead we wanted to use serial execution, let's say we wanted bigFile to print first, then mediumFile even though they take longer to load than smallFile, we'd have to place the callbacks inside each other:

\begin{lstlisting}[style=ES6]
    readFile(bigFile, (err, contents) => {
        print(err, contents)
        readFile(mediumFile, (err, contents) => {
            print(err, contents)
            readFile(smallFile, print)
        })
    })
\end{lstlisting}


Thus, serial execution with callbacks is achieved by waiting for the callback to call before starting the next asynchronous operation.

\subsection{Promises}

A promise is an object that represents an asynchronous operation. It's either pending or settled, and if it is settled it's either resolved or rejected.

Being able to treat an asynchronous operation as an object is a useful abstraction. For instance, instead of passing a function that should be called when an asynchronous operation completes into another function (e.g., a \textit{callback}), a \textit{promise} that represents the asynchronous operation can be returned from a function instead.

This is a callback-based approach:

\begin{lstlisting}[style=ES6]

    function myAsyncOperation (cb) {
        doSomethingAsynchronous((err, value) => { cb(err, value) })
    }

    myAsyncOperation(functionThatHandlesTheResult)

\end{lstlisting}

This is the same in promise form:

\begin{lstlisting}[style=ES6]

    function myAsyncOperation () {
        return new Promise((resolve, reject) => {
            // doSomethingAsynchronous expects a callback
            doSomethingAsynchronous((err, value) => {
                if (err) reject(err)
                else resolve(value)
            })
        })
    }

    const promise = myAsyncOperation()
    // next up: do something with promise


\end{lstlisting}

This gets a lot nicer with the promisify function from the util module:

\begin{lstlisting}[style=ES6]

    const { promisify } = require('util')
    const doSomething = promisify(doSomethingAsynchronous)
    function myAsyncOperation () {
        return doSomething()
    }

    const promise = myAsyncOperation()

\end{lstlisting}

Promise success or failure are handled using \lstinline|then| and \lstinline|catch|:

\begin{lstlisting}[style=ES6]

    const promise = myAsyncOperation()
    // then and catch always return a promise, so they can be chained
    promise
    .then((value) => { console.log(value) })
    .catch((err) => { console.error(err) })

\end{lstlisting}

Below, we have the same readFile operation as in the last section, but the promisify function is used to convert a callback-based API to a promise-based one.

\begin{lstlisting}[style=ES6]

    const { promisify } = require('util')
    const { readFile } = require('fs')

    const readFileProm = promisify(readFile)

    const promise = readFileProm(__filename)

    promise.then((contents) => {
        console.log(contents.toString())
    })

    promise.catch((err) => {
        console.error(err)
    })
\end{lstlisting}

However, using promisify with fs is not necessary, since fs already exports a promises object with promise-based versions. Using this, we can write

\begin{lstlisting}[style=ES6]

    const { readFile } = require('fs').promises

    readFile(__filename)
    .then((contents) => {
        console.log(contents.toString())
    })
    .catch(console.error)
\end{lstlisting}

Here, even though an intermediate promise is created by the first then, we still only need the one catch handler, as rejections are propagated.

Promises also allow for an easy serial execution pattern:

\begin{lstlisting}[style=ES6]

    readFile(bigFile)
    // returns a promise for reading mediumFile
    .then((contents) => {
        print(contents)
        return readFile(mediumFile)
        }}
    //returns a promise for reading smallFile
    .then((contents) => {
        print(contents)
        return readFile(smallFile)
        })
    // returns itself
   .then(print)
   .catch(console.error)

\end{lstlisting}

If parallel execution is desired, \lstinline|Promise.all| can be used to wait for all tasks to be handled. Promise.all takes an array of promises and returns a promise that resolves when all promises have been resolved. That returned promise resolves to an array of the values for each of the promises. This will give the same result of asynchronously reading all the files and concatenating them in a prescribed order.

\begin{lstlisting}[style=ES6]

    const readers = files.map((file) => readFile(file))

    Promise.all(readers)
    .then(print)
    .catch(console.error)

\end{lstlisting}

If one of the promises were to fail, Promise.all would reject, and any successfully resolved promises are ignored. If we want more tolerance of individual errors, \lstinline|Promise.allSettled| can be used:

\begin{lstlisting}[style=ES6]

    const { readFile } = require('fs').promises
    const files = [__filename, 'not a file', __filename]
    const print = (results) => {
        results
        .filter(({status}) => status === 'rejected')
        .forEach(({reason}) => console.error(reason))
        const data = results
        .filter(({status}) => status === 'fulfilled')
        .map(({value}) => value)
        const contents = Buffer.concat(data)
        console.log(contents.toString())
    }

    const readers = files.map((file) => readFile(file))

    Promise.allSettled(readers)
    .then(print)
    .catch(console.error)

\end{lstlisting}

The Promise.allSettled function returns an array of objects representing the settled status of each promise. Each object has a status property, which may be rejected or fulfilled. Objects with a rejected status will contain a reason property containing the error associated with the rejection. Objects with a fulfilled status will have a value property containing the resolved value.

Finally, if we want promises to run in parallel independently, we can either use \lstinline|Promise.allSettled| or simply execute each of them with their own then and catch handlers:

\begin{lstlisting}[style=ES6]
    readFile(bigFile).then(print).catch(console.error)
    readFile(mediumFile).then(print).catch(console.error)
    readFile(smallFile).then(print).catch(console.error)
\end{lstlisting}

\subsection{Async/Await}

An async function always returns a promise. The promise will resolve to whatever is returned inside the async function body.

The await keyword can only be used inside of async functions.Calling await will pause the execution of the async function until the awaited promise is resolved. The resolved value of that promise will be returned from an await expression.

Here's an example of the same readFile operation from the previous section, but this time using an async function:

\begin{lstlisting}[style=ES6]

    const { readFile } = require('fs').promises

    async function run () {
        const contents = await readFile(__filename)
        console.log(contents.toString())
    }

    run().catch(console.error)

\end{lstlisting}

An async function always returns a promise, so we call the catch method to ensure that any rejections within the async function are handled.

This is how serial execution would work with async/await. Both variants print ABC:

\begin{lstlisting}[style=ES6]

    async function run () {
        print(await readFile(bigFile))
        print(await readFile(mediumFile))
        print(await readFile(smallFile))
    }

    run().catch(console.error)

\end{lstlisting}

Here is a serial execution example promisifying a "normal" function:

\begin{lstlisting}[style=ES6]
// see https://yieldcode.blog/post/implementing-promisable-set-timeout/

function setTimeoutPromise(cb, ms) {
    return new Promise((resolve) => {
        setTimeout(() => resolve(cb()), ms);
    });
}

const print = (err, contents) => {
    if (err) console.error(err)
    else console.log(contents )
}

const opA = async (cb) => {
    //setTimeout(() => {
        await setTimeoutPromise(() => {
            cb(null, 'A')
        }, 500)
    }

const opB = async (cb) => {
        await setTimeoutPromise(() => {
            cb(null, 'B')
        }, 250)
    }

const opC = async (cb) => {
        await setTimeoutPromise(() => {
            cb(null, 'C')
        }, 125)
    }

function sleep(delay) {
        return new Promise(resolve => setTimeout(resolve, delay));
    }

const run = async () => {
        // option 1, using then()
        opA(print).then(() => {
            // opC needs to called inside this!!
            opB(print).then(() => {
                opC(print)}
            )
        });

        await sleep(3000);

        // option 2, using await
        await opA(print);
        await opB(print);
        await opC(print);
    }

run()
\end{lstlisting}

If the output only has to be ordered, but the order in which asynchronous operations resolves is immaterial, we can again use \lstinline|Promise.all| but this time await the promise that Promise.all returns:

\begin{lstlisting}[style=ES6]

    async function run () {
        const readers = files.map((file) => readFile(file))
        const data = await Promise.all(readers)
        print(Buffer.concat(data))
    }

    run().catch(console.error)

\end{lstlisting}

To get the exact same parallel operation behavior as in the initial callback example, so that the files are printed as soon as they are loaded, we have to create the promises, use a \lstinline|then| handler and then await the promises later on:

\begin{lstlisting}[style=ES6]

    async function run () {
        const big = readFile(bigFile)
        const medium = readFile(mediumFile)
        const small = readFile(smallFile)

        big.then(print)
        medium.then(print)
        small.then(print)

        await small
        await medium
        await big
    }

    run().catch(console.error)

\end{lstlisting}

This will ensure the contents are printed out chronologically, according to the time it took each of them to load.

To get that behavior with the timeout example, we rewrite the code as follows. Note no then handlers are needed:

\begin{lstlisting}[style=ES6]
    // see https://yieldcode.blog/post/implementing-promisable-set-timeout/

    function setTimeoutPromise(cb, ms) {
        return new Promise((resolve) => {
            setTimeout(() => resolve(cb()), ms);
        });
    }

    const print = (err, contents) => {
        if (err) console.error(err)
        else console.log(contents )
    }

    const opA = async (cb) => {
        //setTimeout(() => {
            await setTimeoutPromise(() => {
                cb(null, 'A')
            }, 500)
        }

        const opB = async (cb) => {
            await setTimeoutPromise(() => {
                cb(null, 'B')
            }, 250)
        }

        const opC = async (cb) => {
            await setTimeoutPromise(() => {
                cb(null, 'C')
            }, 125)
        }

        const run = async () => {

            const A = opA(print);
            const B = opB(print);
            const C = opC(print);

            await A;
            await B;
            await C;

        }

        run()

\end{lstlisting}

If the complexity for parallel execution grows it may be better to use a callback based approach and wrap it at a higher level into a promise so that it can be used in an async/await function:

\begin{lstlisting}[style=ES6]

    const { promisify } = require('util')
    const { readFile } = require('fs')
    const [ bigFile, mediumFile, smallFile ] =
        Array.from(Array(3)).fill(__filename)

    const read = promisify((cb) => {
        let index = 0
        const print = (err, contents) => {
            index += 1
            if (err) {
                console.error(err)
                if (index === 3) cb()
                return
            }
            console.log(contents.toString())
            if (index === 3) cb()
        }
        readFile(bigFile, print)
        readFile(mediumFile, print)
        readFile(smallFile, print)
    })

    async function run () {
        await read()
        console.log('finished!')
    }

    run().catch(console.error)
\end{lstlisting}

Here the read function returns a promise that resolves when all three parallel operations are done.

\subsection{Canceling Asynchronous Operations}

To cancel asynchronous operations, Node core has embraced the  \href{https://developer.mozilla.org/en-US/docs/Web/API/AbortController}{AbortController} with AbortSignal Web APIs.

While AbortController with AbortSignal can be used for callback-based APIs, it's generally used in Node to solve for the fact that promise-based APIs return promises:


\begin{lstlisting}[style=ES6]

    import { setTimeout } from 'timers/promises'

    const ac = new AbortController()
    const { signal } = ac
    const timeout = setTimeout(1000, 'will NOT be logged', { signal })

    setImmediate(() => {
        ac.abort()
    })

    try {
        console.log(await timeout)
    } catch (err) {
        // ignore abort errors:
        if (err.code !== 'ABORT_ERR') throw err
    }

\end{lstlisting}

The AbortController constructor is a global, so we instantiate it and assign it to the ac constant. An AbortController instance has an AbortSignal instance on its signal property. We pass this via the options argument to timers/promises setTimeout; internally the API will listen for an abort event on the signal instance and then cancel the operation if it is triggered.

Many parts of the Node core API accept a signal option, including fs, net, http, events, child\_process, readline and stream.

\section{Event System}

The EventEmitter constructor in the events module is the functional backbone of many Node core API's. For instance, HTTP and TCP servers are an event emitter, a TCP socket is an event emitter, HTTP request and response objects are event emitters. In this chapter, we'll explore how to create and consume EventEmitters.

\subsection{Creating an Event Emitter}

To be able to create an EventEmitter, either import it like this

\begin{lstlisting}[style=ES6]

    // the events module exports an EventEmitter constructor
    import { EventEmitter } from 'node:events';

    // this should work in .cjs files, but yields undefined
    // const { EventEmitter } = require('events')

\end{lstlisting}

or like so:

\begin{lstlisting}[style=ES6]

    // in modern node the events module is
    // the EventEmitter constructor, too :
    import EventEmitter from 'node:events';

    // see events.js:
    /*
    class EventEmitter<T extends EventMap<T> = DefaultEventMap> {
        constructor(options?: EventEmitterOptions);
        // ...
    }

    import internal = require("node:events");
    namespace EventEmitter {
        // Should just be `export { EventEmitter }`,
        // but that doesn't work in TypeScript 3.4
        export { internal as EventEmitter };
        // ...
    */

    // resp.
    const EventEmitter = require('events')

\end{lstlisting}

To create an EventEmitter instance, inherit from the EventEmitter class:

\begin{lstlisting}[style=ES6]

    class MyEmitter extends EventEmitter {}

    const myEmitter = new MyEmitter();
    myEmitter.on('event', () => {
        console.log('an event occurred!');
    });
    myEmitter.emit('event');

\end{lstlisting}

\subsection{Emitting Events}

To emit an event call the emit method:

\begin{lstlisting}[style=ES6]

    const { EventEmitter } = require('events')
    const myEmitter = new EventEmitter()
    // arguments: event namespace, 2/3: passed to listener
    myEmitter.emit('an-event', some, args)

\end{lstlisting}

\subsection{Listening for Events}

\begin{lstlisting}[style=ES6]

    const { EventEmitter } = require('events')

    const ee = new EventEmitter()
    ee.on('close', () => { console.log('close event fired!') })
    ee.emit('close')

\end{lstlisting}

Process additional arguments passed to emit like so:

\begin{lstlisting}[style=ES6]

    ee.on('add', (a, b) => { console.log(a + b) }) // logs 13
    ee.emit('add', 7, 6)

\end{lstlisting}

Listeners are called in the order they are registered. prepedListener is used to add a listener in top position.

\begin{lstlisting}[style=ES6]

    const { EventEmitter } = require('events')
    const ee = new EventEmitter()
    ee.on('my-event', () => { console.log('1st') })
    ee.on('my-event', () => { console.log('2nd') })
    ee.prependListener('my-event', () => { console.log('really 1st') })
    ee.emit('my-event')

\end{lstlisting}

To have a listener called at most once, use eventEmitter.once() instead of on().

\begin{lstlisting}[style=ES6]

    const { EventEmitter } = require('events')
    const ee = new EventEmitter()
    ee.once('my-event', () => { console.log('my-event fired') })
    ee.emit('my-event')
    ee.emit('my-event')
    ee.emit('my-event')

\end{lstlisting}

To remove listeners, use \lstinline|removeListener("eventName", listenerVar)|.

Call \lstinline|removeAllListeners("eventName")| to remove them all.

\begin{lstlisting}[style=ES6]

    const { EventEmitter } = require('events')
    const ee = new EventEmitter()

    const listener1 = () => { console.log('listener 1') }
    const listener2 = () => { console.log('listener 2') }

    ee.on('my-event', listener1)
    ee.on('my-event', listener2)

    setInterval(() => {
        ee.emit('my-event')
    }, 200)

    setTimeout(() => {
        ee.removeListener('my-event', listener1)
    }, 500)

    setTimeout(() => {
        ee.removeListener('my-event', listener2)
    }, 1100)

\end{lstlisting}

\subsection{The error Event}

Emitting an 'error' event on an event emitter will cause the event emitter to throw an exception if a listener for the 'error' event has not been registered.

Here a crash will not occur:

\begin{lstlisting}[style=ES6]

    const { EventEmitter } = require('events')
    const ee = new EventEmitter()

    process.stdin.resume() // keep process alive

    ee.on('error', (err) => {
        console.log('got error:', err.message )
    })

    ee.emit('error', new Error('oh oh'))

\end{lstlisting}

\subsection{Promise-Based Single Use Listener and AbortController}

When \lstinline|await|ing an event, AbortController can be used as an escape hatch.

\begin{lstlisting}[style=ES6]

    import { once, EventEmitter } from 'events'
    import { setTimeout } from 'timers/promises'

    const uneventful = new EventEmitter()

    const ac = new AbortController()
    const { signal } = ac

    setTimeout(500).then(() => ac.abort())

    try {
        await once(uneventful, 'ping', { signal })
        console.log('pinged!')
    } catch (err) {
        // ignore abort errors:
        if (err.code !== 'ABORT_ERR') throw err
        console.log('canceled')
    }

\end{lstlisting}

\section{Handling Errors}
\subsection{Native Error Constructors}

\lstinline|Error| is the native constructor for generating an error object. To create an error, call new Error and pass a string as a message:

\begin{lstlisting}[style=ES6]
    new Error('this is an error message')
\end{lstlisting}

There are six other native error constructors that inherit from the base Error constructor, these are:

\begin{lstlisting}[style=ES6]
    EvalError
    SyntaxError
    RangeError
    ReferenceError
    TypeError
    URIError
\end{lstlisting}

There's mainly two errors that are likely to be thrown in library or application code: RangeError and TypeError.

\subsection{Custom Errors: Set error code on Error object}

\begin{lstlisting}[style=ES6]
    function doTask (amount) {
        if (typeof amount !== 'number')
         throw new TypeError('amount must be a number')
        if (amount <= 0)
         throw new RangeError('amount must be greater than zero')
        if (amount % 2) {
            const err = Error('amount must be even')
            err.code = 'ERR_MUST_BE_EVEN'
            throw err
        }
        return amount / 2
    }
\end{lstlisting}

\subsection{Custom Errors: Inheriting from Error}

\begin{lstlisting}[style=ES6]
    class OddError extends Error {
        constructor (varName = '') {
            super(varName + ' must be even')
        }
        get name () { return 'OddError' }
    }

    // can additionally add a code
    class OddError extends Error {
        constructor (varName = '') {
            super(varName + ' must be even')
            this.code = 'ERR_MUST_BE_EVEN'
        }
        get name () {
            return 'OddError [' + this.code + ']'
        }
    }
\end{lstlisting}

\subsection{Try/Catch}

We can separately handle different kinds of errors like so:

\begin{lstlisting}[style=ES6]
    try {
        const result = doTask(4)
        console.log('result', result)
    } catch (err) {
        if (err instanceof TypeError) {
            console.error('wrong type')
        } else if (err instanceof RangeError) {
            console.error('out of range')
        } else if (err instanceof OddError) {
            console.error('cannot be odd')
        } else {
            console.error('Unknown error', err)
        }
    }
\end{lstlisting}

However, it is more reliable to throw and check for error codes:

\begin{lstlisting}[style=ES6]
    function codify (err, code) {
        err.code = code
        return err
    }

    function doTask (amount) {
        if (typeof amount !== 'number') throw codify(
        new TypeError('amount must be a number'),
        'ERR_AMOUNT_MUST_BE_NUMBER'
        )
        if (amount <= 0) throw codify(
        new RangeError('amount must be greater than zero'),
        'ERR_AMOUNT_MUST_EXCEED_ZERO'
        )
        if (amount % 2) throw new OddError('amount')
        return amount/2
    }

    try {
        const result = doTask(4)
        result()
        console.log('result', result)
    } catch (err) {
        if (err.code === 'ERR_AMOUNT_MUST_BE_NUMBER') {
            console.error('wrong type')
        } else if (err.code === 'ERRO_AMOUNT_MUST_EXCEED_ZERO') {
            console.error('out of range')
        } else if (err.code === 'ERR_MUST_BE_EVEN') {
            console.error('cannot be odd')
        } else {
            console.error('Unknown error', err)
        }
    }
\end{lstlisting}

It's important to keep in mind that try/catch cannot catch errors that are thrown in a callback function that is called at some later point. Consider the following:

\begin{lstlisting}[style=ES6]
    // WARNING: NEVER DO THIS:
    try  {
        setTimeout(() => {
            const result = doTask(3)
            console.log('result', result)
        }, 100)
    } catch (err) {
        // ...
    }
\end{lstlisting}

An easy fix is to move the try/catch into the body of the callback function:

\begin{lstlisting}[style=ES6]
    setTimeout(() => {
        try  {
            const result = doTask(3)
            console.log('result', result)
        } catch (err) {
            // ...
        }
    }, 100)
\end{lstlisting}

\subsection{Rejected Promises}

Assuming a function that accepts a Promise:

\begin{lstlisting}[style=ES6]
    function doTask (amount) {
        return new Promise((resolve, reject) => {
            if (typeof amount !== 'number') {
                reject(new TypeError('amount must be a number'))
                return
            }
            if (amount <= 0) {
                //
            }
            if (amount % 2) {
                //
            }
            resolve(amount/2)
        })
    }
\end{lstlisting}

to catch rejections we have to use a \lstinline|catch| clause:

\begin{lstlisting}[style=ES6]
    doTask(3)
    .then((result) => {
        //
    })
    .catch((err) => {
        //
    })
\end{lstlisting}

When the \lstinline|throw| appears inside a promise handler, that will not be an exception, that is, it won't be an error that is synchronous.
Instead it will be a rejection: The then or catch handler will return a \textit{new promise} that rejects as a result of a throw within the handler.

Example:

\begin{lstlisting}[style=ES6]
    doTask(4)
    .then((result) => {
        throw Error('spanner in the works')
    })
    .catch((err) => {
        //
    })
\end{lstlisting}

Here the catch clause does not run, and an error is thrown.

\subsection{Async Try/Catch}

The same try/catch works for async/await, as async/await is nothing but syntactic sugar around Promise.

For example:

\begin{lstlisting}[style=ES6]
    async function run () {
        try {
            //
        } catch (err) {
            //
        }
    }
\end{lstlisting}

If a throw occurs in an async function, the returned promise rejects. This means that with async/await, we can simply throw again. As a result, the code looks exaxtly like synchronous code, apart from the async keyword itself:

\begin{lstlisting}[style=ES6]
    async function doTask (amount) {
        if (typeof amount !== 'number')
         throw new TypeError('amount must be a number')
        if (amount <= 0)
         throw new RangeError('amount must be greater than zero')
        if (amount % 2) throw new OddError('amount')
        return amount/2
    }
\end{lstlisting}

In consequence, async/await can conveniently be nested:

\begin{lstlisting}[style=ES6]
    async function doTask (amount) {
        if //
        if // ...
        // fetch something from a server
        const result = await asyncFetchResult(amount)
        return result
    }
\end{lstlisting}

    Here, if the promise returned from asyncFetchResult rejects, this will cause the promise returned from doTask to reject.
    Thus, one catch block can be used to handle all cases.


\subsection{Error Propagation}

Error propagation is where, instead of handling the error, we make it the responsibility of the caller instead. Here is an example handling known errors but propagating unknown ones:

\begin{lstlisting}[style=ES6]

    // custom error definitions ....

    async function doTask (amount) {
        if (/* ...  */) throw new TypeError(// ...
        if (/* ...  */) throw new RangeError(// ...
        if (/* ...  */) throw new OddError(// ...
        return amount/2
    }

    async function run () {
        try {
            const result = await doTask(4)
            console.log('result', result)
        } catch (err) {
            if // ...
            } else if // ...
            } else if // ...
            // unknown error
            } else {
                throw err
            }
        }
    }
    // catch remaining errors
    run().catch((err) => { console.error('Error caught', err) })
\end{lstlisting}

Here is the equivalent example using a callback:

\begin{lstlisting}[style=ES6]

    // now takes a callback
    function doTask (amount, cb) {
        if (/* ... */ {
            cb( // .... throw some Error
            return
        }
        if (/* ... */ {
            cb( // .... throw some Error
            return
        }
        if (/* ... */ {
            cb(Error('some other error'))
            return
        }
        // first argument (Error) is null, pass result as second!
        cb(null, amount/2)
    }

    // 1. passes an error-first callback to doTask()
    // 2. later, calls its own caller using the passed-in argument cb
    // beware: cb is not the callback passed to doTask!
    function run (cb) {
        // callback passed to doTask() is created by run()!
        doTask(4, (err, result) => {
            // if error, check code
            // if not a known one pass on as-is
            // first argument (Error) is not null
            if (err) {
                if (err.code === // ...
                    cb(Error('wrong type'))
                } else if (err.code === // ...
                    cb(Error('out of range'))
                } else if // ...
                    cb(Error('cannot be odd'))
                } else {
                    cb(err)
                }
                return
            }

            // do some real stuff with result
            console.log('result', result)
        })
    }

    run((err) => {
        if (err) console.error('Error caught', err);
    })


\end{lstlisting}

\section{Buffers}

A buffer object is both an instance of Buffer and an instance (at the second degree) of Uint8Array (8 bytes of unsigned integers between 0-255; subclass of non-instantiable TypedArray).
In consequence, methods from Uint8Array as well as those from Buffer are available.

To safely create a buffer (meaning, zeroed-out bytes), we use \lstinline|Buffer.alloc(num_bytes)|.

Buffers can conveniently be converted into/from strings:

\begin{lstlisting}[style=ES6]
    \\ not required, but recommended
    const { Buffer } = require('node:buffer');

    const buffer = Buffer.from('hello world')
    buffer

    // <Buffer 68 65 6c 6c 6f 20 77 6f 72 6c 64>

\end{lstlisting}

\subsection{Buffer creation}

The default character set is UTF-8, which uses up to 4 bytes:

\begin{lstlisting}[style=ES6]
    // U+1F440 not displayed by tex
    // length of string: ?!?
    // https://mathiasbynens.be/notes/javascript-unicode
    console.log('<EYES>'.length)
    // 2

    // utf-8 encoding uses 8 bytes
    console.log(Buffer.from('<EYES>').length)
    // 4
    Buffer.from('U+1F440')
    <Buffer f0 9f 91 80>
\end{lstlisting}

But a different character set can be specified:

\begin{lstlisting}[style=ES6]
    Buffer.from("U+00fc")
    <Buffer c3 bc>

    Buffer.from("U+00fc", "ascii")
    <Buffer fc>

    Buffer.from("U+00fc", "utf16le")
    <Buffer fc 00>

\end{lstlisting}

\subsection{Character encodings}

The\textit{ character encodings} currently supported by Node.js are the following:

\begin{itemize}
    \item 'utf8' (alias: 'utf-8')
    \item 'utf16le' (alias: 'utf-16le'): each character in the string will be encoded using either 2 or 4 bytes. Node.js only supports the little-endian variant of UTF-16.
    \item 'latin1': = ISO-8859-1. This character encoding only supports the Unicode characters from U+0000 to U+00FF. Each character is encoded using a single byte.
\end{itemize}

Converting a \textit{Buffer into a string} using one of the above is referred to as \textit{decoding}, and converting \textit{a string into a Buffer} is referred to as \textit{encoding}.

The following legacy character encodings are also supported:

\begin{itemize}
    \item 'ascii': For 7-bit ASCII data only. When encoding a string into a Buffer, this is equivalent to using 'latin1'. When decoding a Buffer into a string, using this encoding will additionally unset the highest bit of each byte before decoding as 'latin1'. Generally, there should be no reason to use this encoding, as 'utf8' (or, if the data is known to always be ASCII-only, 'latin1') will be a better choice when encoding or decoding ASCII-only text. It is only provided for legacy compatibility.
    \item 'binary': Alias for 'latin1'. The name of this encoding can be very misleading, as all of the encodings listed here convert between strings and binary data. For converting between strings and Buffers, typically 'utf8' is the right choice.
    \item 'ucs2', 'ucs-2': Aliases of 'utf16le'. UCS-2 used to refer to a variant of UTF-16 that did not support characters that had code points larger than U+FFFF. In Node.js, these code points are always supported.
\end{itemize}


\subsection{Binary-to-text encodings}

Node.js also supports the following \textit{binary-to-text} encodings. For binary-to-text encodings, the naming convention is reversed: Converting a \textit{Buffer into a string} is typically referred to as \textit{encoding}, and converting \textit{a string into a Buffer} as \textit{decoding}.

\begin{itemize}
    \item 'base64': Base64 encoding. Whitespace characters such as spaces, tabs, and new lines contained within the base64-encoded string are ignored.
    \item 'base64url': base64url encoding as specified in RFC 4648, Section 5.When encoding a Buffer to a string, this encoding will omit padding.
    \item 'hex': Encode each byte as two hexadecimal characters. Data truncation may occur when decoding strings that do not exclusively consist of an even number of hexadecimal characters. See below for an example.
\end{itemize}

Example:

\begin{lstlisting}[style=ES6]
    // utf-8 encoding of ASCII STRING
    Buffer.from("8J+RgA==")
    <Buffer 38 4a 2b 52 67 41 3d 3d>

    // EYES is 8J+RgA== in base64
    // utf-8 encoding of base64 decoding
    Buffer.from("8J+RgA==", "base64")
    <Buffer f0 9f 91 80>

    // same as
    Buffer.from('<EYES>')
    <Buffer f0 9f 91 80>
\end{lstlisting}

\subsection{Decoding to String}

When converting to string, we can also specify an encoding:

\begin{lstlisting}[style=ES6]
    const buffer = Buffer.from('<EYES>')
    console.log(buffer)
    <Buffer f0 9f 91 80>

    console.log(buffer.toString()) // default utf-8
    <EYES>

    console.log(buffer.toString('hex'))
    f09f9180

    console.log(buffer.toString('base64'))
    8J+RgA==console.log(buffer.toString(')) // prints 8J+RgA==
\end{lstlisting}



\subsection{Using StringDecoder}
The UTF8 encoding format has between 1 and 4 bytes to represent each character, if for any reason one or more bytes is truncated from a character this will result in encoding errors. So in situations where we have multiple buffers that might split characters across a byte boundar y the Node core string\_decoder module should be used.

 Calling decoder.write will output a character only when all of the bytes representing that character have been written to the decoder:

\begin{lstlisting}[style=ES6]
   const { StringDecoder } = require('string_decoder')

   const frag1 = Buffer.from('f09f', 'hex')
   const frag2 = Buffer.from('9180', 'hex')

   console.log(frag1.toString())
   <question_mark>

   console.log(frag2.toString())
    <question_mark>

   const decoder = new StringDecoder()

   console.log(decoder.write(frag1)) // prints nothing
   console.log(decoder.write(frag2)) // prints <EYES>
   <EYES>

\end{lstlisting}

\subsection{JSON Serializing and Deserializing Buffers}

 When JSON.stringify encounters any object it will attempt to call a toJSON method on that object if it exists. Buffer instances have a toJSON method which returns a plain JavaScript object in order to represent the buffer in a JSON-friendly way:

\begin{lstlisting}[style=ES6]
    buffer
    <Buffer f0 9f 91 80>

    buffer.toJSON()
    { type: 'Buffer', data: [ 240, 159, 145, 128 ] }

    JSON.stringify(buffer)
    '{"type":"Buffer","data":[240,159,145,128]}'
\end{lstlisting}

When deserializing, JSON.parse will only turn that JSON representation of the buffer into a plain JavaScript object, to turn it into an object the data array must be passed to Buffer.from:

\begin{lstlisting}[style=ES6]
    const buffer = Buffer.from('<EYES>')
    const json = JSON.stringify(buffer)

    const parsed = JSON.parse(json)
    console.log(parsed)
    { type: 'Buffer', data: [ 240, 159, 145, 128 ] }

    console.log(Buffer.from(parsed.data))
    <Buffer f0 9f 91 80>
\end{lstlisting}

When an array of numbers is passed to Buffer.from they are converted to a buffer with byte values corresponding to those numbers.

\subsection{Example}


\begin{lstlisting}[style=ES6]
    const str = 'buffers are neat'

    // <Buffer 62 75 66 66 65 72 73 20 61 72 65 20 6e 65 61 74>
    let y = Buffer.from(str)
    // 'YnVmZmVycyBhcmUgbmVhdA=='
    let z = y.toString('base64')

    // 1liner
    // const base64 = Buffer.from(str).toString('base64')

    assert.equal(z, Buffer.from([
    89,110,86,109,90,109,86,121,99,
    121,66,104,99,109,85,103,98,109,
    86,104,100,65,61,61]))
\end{lstlisting}

\section{Streams}
\subsection{Stream Types}

Constructors exposed by the base \textit{stream} module are:

\begin{lstlisting}[style=ES6]
    Stream
    Readable
    Writable
    Duplex
    Transform
    PassThrough
\end{lstlisting}

The Stream constructor inherits from the \textit{EventEmitter} constructor from the \textit{events} module.

Events emitted by various Stream implementations are:

\begin{lstlisting}[style=ES6]
    data
    end
    finish
    close
    error
\end{lstlisting}

\subsection{Stream Modes}

The mode of a stream is determined by its \textit{objectMode} option passed when the stream is instantiated, the default being binary. \textit{Binary mode} streams only read or write \textit{Buffer} instances. \textit{object mode} streams read or write \textit{objects and primitives} (strings, numbers) except null.

\subsection{Readable Streams}

To create, using fs' \textit{createReadStream} constructor:

\begin{lstlisting}[style=ES6]
 'use strict'
 const fs = require('fs')

 // file executing the code
 const readable = fs.createReadStream(__filename)

 readable.on('data', (data) => { console.log(' got data', data) })
 readable.on('end', () => { console.log(' finished reading') })

\end{lstlisting}


Readable streams are usually connected to an I/O layer via a C-binding, but we can create a contrived readable stream ourselves using the Readable constructor:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { Readable } = require('stream')

    const createReadStream = () => {
        const data = ['some', 'data', 'to', 'read']
        // pass an options object with a read method
        return new Readable({
            // ca pass size (how many bytes to read),
            // highWaterMark (default 16kb)
            read () {
                // this points to readable stream instance
                if (data.length === 0) this.push(null)
                else this.push(data.shift())
            }
        })
    }

    const readable = createReadStream()

    readable.on('data', (data) => { console.log('got data', data) })
    readable.on('end', () => { console.log('finished reading') })
\end{lstlisting}

When each data event is emitted it receives a string instead of a buffer.
However, the default stream mode is objectMode: false, meaning Buffer.


Thus, a string is pushed to the Readable stream. That stream is then converted to a buffer. Finally, it is decoded to string using UTF8.

When creating a readable stream without the intention of using buffers, we can instead set objectMode to true:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { Readable } = require('stream')

    const createReadStream = () => {
        const data = ['some', 'data', 'to', 'read']
        return new Readable({
            objectMode: true,
            read () {
                if (data.length === 0) this.push(null)
                else this.push(data.pop())
            }
        })
    }

    const readable = createReadStream()

    readable.on('data', (data) => { console.log('got data', data) })
    readable.on('end', () => { console.log('finished reading') })
\end{lstlisting}

This time the string is being sent from the readable stream without converting to a buffer first.

Using \textit{Readable.from}, streams can be created from iterable data structures, like arrays:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { Readable } = require('stream')

    // objectMode is true by default
    const readable = Readable.from(['some', 'data', 'to', 'read'])

    readable.on('data', (data) => { console.log('got data', data) })
    readable.on('end', () => { console.log('finished reading') })

\end{lstlisting}

\subsection{Writable Streams}

To create:

\begin{lstlisting}[style=ES6]
'use strict'
const fs = require('fs')

const writable = fs.createWriteStream('./out')
writable.on('finish', () => { console.log('finished writing') })

// strings are converted to buffers
writable.write('A\n')
writable.write('B\n')
writable.write('C\n')
writable.end('nothing more to write')
\end{lstlisting}

Like Readable streams, Writable streams are mostly useful for I/O, which means integrating a writable stream with a native C-binding, but we can likewise create a contrived write stream example:

\begin{lstlisting}[style=ES6]
'use strict'
const { Writable } = require('stream')

const createWriteStream = (data) => {
    return new Writable({
        write (chunk, enc, next) {
            data.push(chunk)
            // we are ready for the next piece of data
            next()
        }
    })
}
const data = []

const writable = createWriteStream(data)
writable.on('finish', () => { console.log('finished writing', data) })

writable.write('A\n')
writable.write('B\n')
writable.write('C\n')
writable.end('nothing more to write')

\end{lstlisting}

The point of a next callback function is to allow for asynchronous operations within the write. This is essential for performing asynchronous I/O.

Note again, as with readable streams, the default objectMode option is false, so each string written to our writable stream instance is converted to a buffer before it becomes the chunk argument passed to the write option function.

If we are dealing with strings or Buffers only, we can avoid the unnecessary conversion by setting the decodeStrings option to false:

\begin{lstlisting}[style=ES6]
const createWriteStream = (data) => {
    return new Writable({
        decodeStrings: false,
        write (chunk, enc, next) {
            data.push(chunk)
            next()
        }
    })
}
\end{lstlisting}

Now trying to pass anything else (e.g., a number) will result in an error because we're attempting to write a JavaScript value that isn't a string to a binary stream.

If instead we want to support strings and any other JavaScript value, we can instead set objectMode to true to create an object-mode writable stream:

\begin{lstlisting}[style=ES6]
'use strict'
const { Writable } = require('stream')

const createWriteStream = (data) => {
    return new Writable({
        objectMode: true,
        write (chunk, enc, next) {
            data.push(chunk)
            next()
        }
    })
}
const data = []

const writable = createWriteStream(data)
writable.on('finish', () => { console.log('finished writing', data) })
writable.write('A\n')
writable.write(1)
writable.end('nothing more to write')
\end{lstlisting}

\subsection{Readable-Writable Streams}

In addition to the Readable and Writable stream constructors there are three more core stream constructors that have both readable and writable interfaces: Duplex, Transform, and PassThrough.

\subsubsection{Duplex}

With a Duplex stream, both read and write methods are implemented but there doesn't have to be a causal relationship between them. Just because something is written to a Duplex stream doesn't necessarily mean that it will result in any change to what can be read from the stream, although it might.

A TCP network socket is a great example of a Duplex stream:

\begin{lstlisting}[style=ES6]
    'use strict'
    const net = require('net')

    net.createServer((socket) => {
        const interval = setInterval(() => {
            // writable side of the stream
            socket.write('beat')
        }, 1000)
        // readable side of the stream
        socket.on('data', (data) => {
            // writable side of the stream
            socket.write(data.toString().toUpperCase())
        })
        // readable side of the stream
        socket.on('end', () => { clearInterval(interval) })
    }).listen(3000)
\end{lstlisting}

The client socket is also a Duplex stream:

\begin{lstlisting}[style=ES6]
    'use strict'
    const net = require('net')

    const socket = net.connect(3000)

    socket.on('data', (data) => {
        console.log('got data:', data.toString())
    })

    socket.write('hello')

    setTimeout(() => {
        socket.write('all done')
        setTimeout(() => {
            socket.end()
        }, 250)
    }, 3250)
\end{lstlisting}

\subsubsection{Transform}

The Transform constructor inherits from the Duplex constructor. Transform streams are duplex streams with an additional constraint applied to enforce a causal relationship between the read and write interfaces. A good example is compression:


\begin{lstlisting}[style=ES6]

    'use strict'
    const { createGzip } = require('zlib')

    const transform = createGzip()

    transform.on('data', (data) => {
        console.log('got gzip data', data.toString('base64'))
    })
    transform.write('first')
    setTimeout(() => {
        transform.end('second')
    }, 500)
\end{lstlisting}

The way that Transform streams create this causal relationship is through how a transform stream is created. Instead of supplying read and write options functions, a transform option is passed to the Transform constructor:


\begin{lstlisting}[style=ES6]
'use strict'
const { Transform } = require('stream')
const { scrypt } = require('crypto')

const createTransformStream = () => {
    return new Transform({
        decodeStrings: false,
        encoding: 'hex',
        // same signature as write but
        // the next function can be passed a second argument
        // which should be the result of applying some kind
        // of transform operation to the incoming chunk.
        transform (chunk, enc, next) {
            scrypt(chunk, 'a-salt', 32, (err, key) => {
                if (err) {
                    // emit an error event
                    next(err)
                    return
                }
                // null indicates no error,
                // and data event is emitted from the readable side
                next(null, key)
            })
        }
    })
}
const transform = createTransformStream()
transform.on('data', (data) => {
    console.log('got data:', data)
})
transform.write('A\n')
transform.write('B\n')
transform.write('C\n')
transform.end('nothing more to write')

\end{lstlisting}

\subsubsection{PassThrough}

The PassThrough constructor inherits from the Transform constructor. It's essentially a transform stream where no transform is applied. It implements the identity function, that is, it's a useful placeholder when a transform stream is expected but no transform is desired.

\subsubsection{Determining End-of-Stream}

As we discussed earlier, there are at least four ways for a stream to potentially become inoperative: close event, error event, finish event, and end event.

We often need to know when a stream has closed so that resources can be deallocated, otherwise memory leaks become likely.

Instead of listening to all four events, the stream.finished utility function provides a simplified way to do this:

\begin{lstlisting}[style=ES6]
'use strict'
const net = require('net')
const { finished } = require('stream')

net.createServer((socket) => {
    const interval = setInterval(() => {
        socket.write('beat')
    }, 1000)
    socket.on('data', (data) => {
        socket.write(data.toString().toUpperCase())
    })
    // The stream (socket) is passed to finished as the first argument
    // the second argument is a callback for when the stream ends
    // for any reason
    finished(socket, (err) => {
        // first argument of the callback is a potential error object
        // If the stream were to emit an error event the callback would
        // be called with the error object emitted by that event.
        if (err) {
            console.error('there was a socket error', err)
        }
        clearInterval(interval)
    })
}).listen(3000)

\end{lstlisting}

\subsubsection{Piping Streams}

Let's adapt the TCP client server from the "Readable-Writable Streams" page to use the pipe method. Here is the client from earlier:

\begin{lstlisting}[style=ES6]
    'use strict'
    const net = require('net')

    const socket = net.connect(3000)

    socket.on('data', (data) => {
        console.log('got data:', data.toString())
    })

    socket.write('hello')
    setTimeout(() => {
        socket.write('all done')
        setTimeout(() => {
            socket.end()
        }, 250)
    }, 3250)
\end{lstlisting}

We'll replace the data event listener with a pipe:

\begin{lstlisting}[style=ES6]
    'use strict'
    const net = require('net')

    const socket = net.connect(3000)

    socket.pipe(process.stdout)

    socket.write('hello')
    setTimeout(() => {
        socket.write('all done')
        setTimeout(() => {
            socket.end()
        }, 250)
    }, 3250)
\end{lstlisting}

The pipe method exists on Readable streams (recall socket is a Duplex stream instance and that Duplex inherits from Readable), and is passed a Writable stream (or a stream with Writable capabilities). Internally, the pipe method sets up a data listener on the readable stream and automatically writes to the writable stream as data becomes available.

Since pipe returns the stream passed to it, it is possible to chain pipe calls together: streamA.pipe(streamB).pipe(streamC). This is a commonly observed practice, but it's also bad practice to create pipelines this way. If a stream in the middle fails or closes for any reason, the other streams in the pipeline will not automatically close. This can create severe memory leaks and other bugs.

The correct way to pipe multiple streams is to use the stream.pipeline utility function.

Combining the above Transform stream and the TCP server to create a pipeline of streams:

\begin{lstlisting}[style=ES6]
    'use strict'
    const net = require('net')
    const { Transform, pipeline } = require('stream')
    const { scrypt } = require('crypto')
    const createTransformStream = () => {
        return new Transform({
            decodeStrings: false,
            encoding: 'hex',
            transform (chunk, enc, next) {
                scrypt(chunk, 'a-salt', 32, (err, key) => {
                    if (err) {
                        next(err)
                        return
                    }
                    next(null, key)
                })
            }
        })
    }

    net.createServer((socket) => {
        const transform = createTransformStream()
        const interval = setInterval(() => {
            socket.write('beat')
        }, 1000)
        pipeline(socket, transform, socket, (err) => {
            if (err) {
                console.error('there was a socket error', err)
            }
            clearInterval(interval)
        })
    }).listen(3000)

\end{lstlisting}

The pipeline command will call pipe on every stream passed to it, and will allow a function to be passed as the final function. Note how we removed the finished utility method. This is because the final function passed to the pipeline function will be called if any of the streams in the pipeline close or fail for any reason.

\section{Interacting with the File System}
\subsection{File Paths}

\begin{lstlisting}[style=ES6]
    // absolute file paths
    console.log('current filename', __filename)
    console.log('current dirname', __dirname)
\end{lstlisting}

The path.join method generates platform-independent paths.

\begin{lstlisting}[style=ES6]
    'use strict'
    const { join } = require('path')
    console.log('out file:', join(__dirname, 'out.txt'))
\end{lstlisting}

Alongside path.join the other path builders are:



\begin{lstlisting}[style=ES6]

    // Given two absolute paths, calculates the relative path between them.
    path.relative

    // e.g. path.resolve('/foo', 'bar', 'baz') yields '/foo/bar/baz',
    // which is akin to executing cd /foo then cd bar then cd baz
    path.resolve

    // Resolves .. and . dot in paths and strips extra slashes, for
    // instance path.normalize('/foo/../bar//baz') returns '/bar/baz
    path.normalize

    path.format
    // Builds a string from an object. The object shape 'that path.format
    // accepts, corresponds to the object returned from path.parse
\end{lstlisting}

The path deconstructors are path.parse, path.extname, path.dirname and path.basename.

\begin{lstlisting}[style=ES6]
    'use strict'
    const { parse, basename, dirname, extname } = require('path')
    console.log('filename parsed:', parse(__filename))
    console.log('filename basename:', basename(__filename))
    console.log('filename dirname:', dirname(__filename))
    console.log('filename extname:', extname(__filename))
\end{lstlisting}

\subsection{Reading and Writing}

The fs module has lower level and higher level APIs. The lower level API's closely mirror POSIX system calls. For instance, fs.open opens and possibly creates a file and provides a file descriptor handle, taking same options as the POSIX open command.

The higher level methods for reading and writing are provided in four abstraction types: Synchronous, callback-based, promise-based, and stream-based.

\subsubsection{Synchronous}

E.g.

\begin{lstlisting}[style=ES6]
    'use strict'
    const { readFileSync } = require('fs')

    const contents = readFileSync(__filename)
    // or set encoding
    const contents = readFileSync(__filename, {encoding: 'utf8'})

    console.log(contents)
\end{lstlisting}

The fs.writeFileSync function takes a path and a string or buffer and blocks the process until the file has been completely written:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { join } = require('path')

    const { readFileSync, writeFileSync } = require('fs')
    const contents = readFileSync(__filename, {encoding: 'utf8'})
    writeFileSync(join(__dirname, 'out.txt'), contents.toUpperCase())
    // or
    writeFileSync(join(__dirname, 'out.txt'), contents.toUpperCase(), {
        flag: 'a'
    })

\end{lstlisting}

\subsubsection{Callback-based}

Read:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { readFile } = require('fs')

    readFile(__filename, {encoding: 'utf8'}, (err, contents) => {
        if (err) {
            console.error(err)
            return
        }
        console.log(contents)
    })
\end{lstlisting}

Read and write, asynchronously each:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { join } = require('path')
    const { readFile, writeFile } = require('fs')

    readFile(__filename, {encoding: 'utf8'}, (err, contents) => {
        if (err) {
            console.error(err)
            return
        }
        const out = join(__dirname, 'out.txt')
        writeFile(out, contents.toUpperCase(), (err) => {
            if (err) { console.error(err) }
        })
    })
\end{lstlisting}

\subsubsection{Promise-based}

The fs/promises API provides most of the same asynchronous methods that are available on fs, but the methods return promises instead of accepting callbacks.

So instead of loading readFile and writeFile like so:

\begin{lstlisting}[style=ES6]
    const { readFile, writeFile } = require('fs')
\end{lstlisting}

We can load the promise-based versions like so:

\begin{lstlisting}[style=ES6]
    const { readFile, writeFile } = require('fs/promises')
\end{lstlisting}

It's also possible to load fs/promises with

\begin{lstlisting}[style=ES6]
    require('fs').promises
\end{lstlisting}

, which is backwards compatible with legacy Node versions (v12 and v10), but fs/promises supersedes fs.promises and aligns with other more recent API additions (such as stream/promises and timers/promises).

Let's look at the same reading and writing example using fs/promises and using async/await to resolve the promises:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { join } = require('path')
    const { readFile, writeFile } = require('fs/promises')

    async function run () {
        const contents = await readFile(__filename, {encoding: 'utf8'})
        const out = join(__dirname, 'out.txt')
        await writeFile(out, contents.toUpperCase())
    }

    run().catch(console.error)
\end{lstlisting}

\subsubsection{File Streams}

The fs module has fs.createReadStream and fs.createWriteStream methods which allow us to read and write files in chunks. Streams are ideal when handling very large files that can be processed incrementally.

Let's start by simply copying the file:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { pipeline } = require('stream')
    const { join } = require('path')
    const { createReadStream, createWriteStream } = require('fs')

    pipeline(
    createReadStream(__filename),
    createWriteStream(join(__dirname, 'out.txt')),
    (err) => {
        if (err) {
            console.error(err)
            return
        }
        console.log('finished writing')
    }
    )
\end{lstlisting}

To reproduce the read, upper-case, write scenario we created in the previous section, we will need a transform stream to upper-case the content:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { pipeline } = require('stream')
    const { join } = require('path')
    const { createReadStream, createWriteStream } = require('fs')
    const { Transform } = require('stream')
    const createUppercaseStream = () => {
        return new Transform({
            transform (chunk, enc, next) {
                const uppercased = chunk.toString().toUpperCase()
                next(null, uppercased)
            }
        })
    }

    pipeline(
    createReadStream(__filename),
    createUppercaseStream(),
    createWriteStream(join(__dirname, 'out.txt')),
    (err) => {
        if (err) {
            console.error(err)
            return
        }
        console.log('finished writing')
    }
    )
\end{lstlisting}

\subsubsection{Reading Directories}

Directories are a special type of file, which hold a catalog of files. Similar to files the fs module provides multiple ways to read a directory: synchronous, callback-based, promise-based, and an async iterable that inherits from fs.Dir.

 Let's look at synchronous, callback-based and promise-based at the same time:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { readdirSync, readdir } = require('fs')
    const { readdir: readdirProm } = require('fs/promises')

    // synchronous method may throw so wrap in try/catch
    try {
        console.log('sync', readdirSync(__dirname))
    } catch (err) {
        console.error(err)
    }

    // callback
    readdir(__dirname, (err, files) => {
        if (err) {
            console.error(err)
            return
        }
        console.log('callback', files)
    })

    async function run () {
        const files = await readdirProm(__dirname)
        console.log('promise', files)
    }

    // If readdirProm does reject, run will likewise reject
    // This is why a catch handler is attached
    run().catch((err) => {
        console.error(err)
    })

\end{lstlisting}

Example: streaming directory contents over HTTP in JSON format:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { createServer } = require('http')
    const { Readable, Transform, pipeline } = require('stream')
    const { opendir } = require('fs')


    const createEntryStream = () => {
        let syntax = '[\n'

        return new Transform({
            // true because dirStream is an object stream
            writableObjectMode: true,
            // false because res is a binary stream
            readableObjectMode: false,
            transform (entry, enc, next) {
                next(null, `${syntax} "${entry.name}"`)
                syntax = ',\n'
            },
            final (cb) {
                this.push('\n]\n')
                cb()
            }
        })
    }

    createServer((req, res) => {
        if (req.url !== '/') {
            res.statusCode = 404
            res.end('Not Found')
            return
        }

        opendir(__dirname, (err, dir) => {
            if (err) {
                res.statusCode = 500
                res.end('Server Error')
                return
            }
            // dir not a stream, but an async iterable
            const dirStream = Readable.from(dir)
            const entryStream = createEntryStream()
            res.setHeader('Content-Type', 'application/json')
            // pipeline from dirStream to entryStream to res
            // passing a final callback to pipeline
            pipeline(dirStream, entryStream, res, (err) => {
                if (err) console.error(err)
            })
        })
    }).listen(3000)
\end{lstlisting}


\subsection{File Metadata}

Metadata about files can be obtained with the following methods:


\begin{lstlisting}[style=ES6]
    fs.stat
    fs.statSync
    fs/promises stat

    fs.lstat
    fs.lstatSync
    fs/promises lstat
\end{lstlisting}

The only difference between the stat and lstat methods is that stat follows symbolic links, and lstat will get metadata for symbolic links instead of following them.

These methods return an fs.Stat instance which has a variety of properties and methods for looking up metadata about a file.

Let's start by reading the current working directory and finding out whether each entry is a directory or not.

\begin{lstlisting}[style=ES6]
    'use strict'
    const { readdirSync, statSync } = require('fs')

    const files = readdirSync('.')

    for (const name of files) {
        const stat = statSync(name)
        const typeLabel = stat.isDirectory() ? 'dir: ' : 'file: '
        console.log(typeLabel, name)
    }

\end{lstlisting}

Let's extend our example with time stats. There are four stats available for files:

\begin{itemize}
    \item Access time
    \item Change time
    \item Modified time
    \item Birth time
\end{itemize}

The difference between change time and modified time, is modified time only applies to writes (although it can be manipulated by fs.utime), whereas change time applies to writes and any status changes such as changing permissions or ownership.

With default options, the time stats are offered in two formats, one is a Date object and the other is milliseconds since the epoch. We'll use the Date objects and convert them to locale strings.

Let's update our code to output the four different time stats for each file:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { readdirSync, statSync } = require('fs')

    const files = readdirSync('.')

    for (const name of files) {
        const stat = statSync(name)
        const typeLabel = stat.isDirectory() ? 'dir: ' : 'file: '
        const { atime, birthtime, ctime, mtime } = stat
        console.group(typeLabel, name)
        console.log('atime:', atime.toLocaleString())
        console.log('ctime:', ctime.toLocaleString())
        console.log('mtime:', mtime.toLocaleString())
        console.log('birthtime:', birthtime.toLocaleString())
        console.groupEnd()
        console.log()
    }
\end{lstlisting}

\subsection{Watching}

The fs.watch method is provided by Node core to tap into file system events. It is, however, fairly low level and not the most friendly of APIs. It's worth considering the ecosystem library, chokidar for file watching needs as it provides a friendlier high level API.

Example use:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { watch } = require('fs')

    watch('.', (evt, filename) => {
        console.log(evt, filename)
    })
\end{lstlisting}

\section{Process and Operating System}

\subsection{STDIO}

Example:

\begin{lstlisting}[style=ES6]
    'use strict'
    console.log('initialized')
    const { Transform } = require('stream')

    const createUppercaseStream = () => {
        return new Transform({
            transform (chunk, enc, next) {
                const uppercased = chunk.toString().toUpperCase()
                next(null, uppercased)
            }
        })
    }

    const uppercase = createUppercaseStream()

    process.stdin.pipe(uppercase).pipe(process.stdout)
\end{lstlisting}

\subsection{Process Info}

Among others:

\begin{lstlisting}[style=ES6]
    'use strict'

    console.log('Current Directory', process.cwd())
    console.log('Process Platform', process.platform)
    console.log('Process ID', process.pid)
\end{lstlisting}

To get the environment variables we can use process.env, and query for specific keys:
To set environment variables, create a new key.

\begin{lstlisting}[style=ES6]
    process.env.MODULEPATH
    /etc/modulefiles:/usr/share/modulefiles:/usr/share/modulefiles/Linux:
    /usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core

    process.env.FOO='my env var'
\end{lstlisting}

\subsection{Process Stats}

The process object has methods which allow us to query resource usage. We're going to look at the process.uptime(), process.cpuUsage and process.memoryUsage functions.

Usage:

\begin{lstlisting}[style=ES6]
    'use strict'

    setTimeout(() => {
        const uptime = process.uptime()
        const { user, system } = process.cpuUsage()
        const stats = [process.memoryUsage()]

        console.log(uptime, user, system, (user + system)/1000000)
    }, 1000)


\end{lstlisting}

\subsection{System Info}

The os module can be used to get information about the Operating System.

\begin{lstlisting}[style=ES6]
  'use strict'
  const os = require('os')

  console.log('Hostname', os.hostname())
  console.log('Home dir', os.homedir())
  console.log('Temp dir', os.tmpdir())
\end{lstlisting}

There are two ways to identify the Operating System with the os module: the os.platform function which returns the same as process.platform property, and the os.type function which on non-Windows systems uses the uname command and on Windows it uses the ver command.

\begin{lstlisting}[style=ES6]
    'use strict'
    const os = require('os')

    console.log('platform', os.platform())
    console.log('type', os.type())
\end{lstlisting}

\subsection{System Stats}

\begin{lstlisting}[style=ES6]
    'use strict'
    const os = require('os')

    setInterval(() => {
        console.log('system uptime', os.uptime())
        console.log('freemem', os.freemem())
        console.log('totalmem', os.totalmem())
        console.log()
    }, 1000)
\end{lstlisting}

\section{Child processes}

\subsection{Creating Child Processes}

\subsubsection{execFile and execFileSync Methods}

The execFile and execFileSync methods are variations of the exec and execSync methods. Rather than defaulting to executing a provided command in a shell, it attempts to execute the provided path to a binary executable directly.

\subsubsection{fork}

The fork method is a specialization of the spawn method. By default, it will spawn a new Node process of the currently executing JavaScript file (although a different JavaScript file to execute can be supplied). It also sets up Interprocess Communication (IPC) with the subprocess by default.

\subsubsection{exec and execSync Methods}

The child\_process.execSync method is the simplest way to execute a command.

\begin{lstlisting}[style=ES6]
    'use strict'

    const { execSync } = require('child_process')
    const output = execSync(
    `node -e "console.log('subprocess stdio output')"`
    )
    console.log(output.toString())
\end{lstlisting}

The execSync method returns a buffer containing the output (from STDOUT) of the command.

If we do want to execute the node binary as a child process, it's best to refer to the full path of the node binary of the currently executing Node process. This can be found with process.execPath. Using process.execPath ensures that no matter what, the subprocess will be executing the same version of Node.

The following is the same example from earlier, but using process.execPath in place of just 'node':

\begin{lstlisting}[style=ES6]
    'use strict'
    const { execSync } = require('child_process')
    const output = execSync(
    `${process.execPath} -e "console.error('subprocess stdio output')"`
    )
    console.log(output.toString())
\end{lstlisting}

The exec method takes a shell command as a string and executes it the same way as execSync. Unlike execSync the asynchronous exec function splits the STDOUT and STDERR output by passing them as separate arguments to the callback function:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { exec } = require('child_process')

    exec(
    `"${process.execPath}" -e "console.log('A');console.error('B')"`,
    (err, stdout, stderr) => {
        console.log('err', err)
        console.log('subprocess stdout: ', stdout.toString())
        console.log('subprocess stderr: ', stderr.toString())
    }
    )
\end{lstlisting}

\subsubsection{spawn and spawnSync Methods}

While exec and execSync take a full shell command, spawn takes the executable path as the first argument and then an array of flags that should be passed to the command as second argument:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawnSync } = require('child_process')

    const result = spawnSync(
    //  executable path
    process.execPath,
    // array of flags to be passed to spawned process
    ['-e', `console.log('subprocess stdio output')`]
    )
    // While the execSync function returns a buffer
    // containing the child process output,
    // the spawnSync function returns an object containing
    // information about the process that was spawned.
    console.log(result)
\end{lstlisting}

Unlike execSync, the spawnSync method does not need to be wrapped in a try/catch. If a spawnSync process exits with a non-zero exit code, it does not throw:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawnSync } = require('child_process')
    const result = spawnSync(process.execPath, [`-e`, `process.exit(1)`])
    console.log(result)
\end{lstlisting}

Just as there are differences between execSync and spawnSync there are differences between exec and spawn.

While exec accepts an optional callback, spawn does not. Both exec and spawn return a ChildProcess instance however, which has stdin, stdout and stderr streams and inherits from EventEmitter allowing for exit code to be obtained after a close event is emitted:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawn } = require('child_process')

    const sp = spawn(
    process.execPath,
    [`-e`, `console.log('subprocess stdio output')`]
    )

    console.log('pid is', sp.pid)

    // get the STDOUT of the child process
    sp.stdout.pipe(process.stdout)

    sp.on('close', (status) => {
        console.log('exit status was', status)
    })
\end{lstlisting}

There is one highly important differentiator between spawn and the other three methods we've been exploring (namely exec, execSync and spawnSync): the spawn method is the only method of the four that\textit{ doesn't buffer child process output}.

Even though the exec method has stdout and stderr streams, they will stop streaming once the subprocess output has reached 1 mebibyte (or 1024 * 1024 bytes). This can be configured with a maxBuffer option, but no matter what, the other three methods have an upper limit on the amount of output a child process can generate before it is truncated.

Since the spawn method does not buffer at all, it will continue to stream output for the entire lifetime of the subprocess, no matter how much output it generates. Therefore, for long running child processes it's recommended to use the spawn method.

\subsection{Process Configuration}

An options object can be passed as a third argument in the case of spawn and spawnSync or the second argument in the case of exec and execSync.

We'll explore two options that can be passed which control the environment of the child process: cwd and env.

We'll use spawn for our example but these options are universally the same for all the child creation methods.

By default, the child process inherits the environment variables of the parent process:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawn } = require('child_process')

    process.env.A_VAR_WE = 'JUST SET'
    const sp = spawn(process.execPath, ['-p', 'process.env'])
    sp.stdout.pipe(process.stdout)
\end{lstlisting}

If we pass an options object with an env property the parent environment variables will be overwritten:

\begin{lstlisting}[style=ES6]
    'use strict'

    const { spawn } = require('child_process')

    process.env.A_VAR_WE = 'JUST SET'

    const sp = spawn(process.execPath, ['-p', 'process.env'], {
        env: {SUBPROCESS_SPECIFIC: 'ENV VAR'}
    })

    sp.stdout.pipe(process.stdout)
\end{lstlisting}

Another option that can be set when creating a child process is the cwd option:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { IS_CHILD } = process.env

    if (IS_CHILD) {
        console.log('Subprocess cwd:', process.cwd())
        console.log('env', process.env)
    } else {
        const { parse } = require('path')
        const { root } = parse(process.cwd())
        const { spawn } = require('child_process')
        const sp = spawn(process.execPath, [__filename], {
            cwd: root,
            env: {IS_CHILD: '1'}
        })

        sp.stdout.pipe(process.stdout)
    }
\end{lstlisting}

\subsection{Child STDIO}

So far we've covered that the asynchronous child creation methods (exec and spawn) return a ChildProcess instance which has stdin, stdout and stderr streams representing the I/O of the subprocess.

This is the default behavior, but it can be altered.

Let's start with an example with the default behavior:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawn } = require('child_process')
    const sp = spawn(
    process.execPath,
    [
    '-e',
    `console.error('err output'); process.stdin.pipe(process.stdout)`
    ],
    // default
    { stdio: ['pipe', 'pipe', 'pipe'] }
    )

    sp.stdout.pipe(process.stdout)
    sp.stderr.pipe(process.stdout)
    sp.stdin.write('this input will become output\n')
    // ends the input stream, allowing the child process to exit
    // which in turn allows the parent process to exit.
    sp.stdin.end()
\end{lstlisting}

If we're piping the subprocess STDOUT to the parent process STDOUT without transforming the data in any way, we can instead set the second element of the stdio array to 'inherit'. This will cause the child process to inherit the STDOUT of the parent:


We've changed the stdio[1] element from 'pipe' to 'inherit' and . This will result in the exact same output:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawn } = require('child_process')
    const sp = spawn(
    process.execPath,
    [
    '-e',
    `console.error('err output'); process.stdin.pipe(process.stdout)`
    ],
    { stdio: ['pipe', 'inherit', 'pipe'] }
    )

    sp.stderr.pipe(process.stdout)
    // removed the sp.stdout.pipe(process.stdout) line
    // (in fact sp.stdout would now be null)
    sp.stdin.write('this input will become output\n')
    sp.stdin.end()
\end{lstlisting}

The stdio option can also be passed a stream directly. In our example, we're still piping the child process STDERR to the parent process STDOUT. Since process.stdout is a stream, we can set stdio[2] to process.stdout to achieve the same effect:



Now both sp.stdout and sp.stderr will be null because neither of them are configured to 'pipe' in the stdio option.

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawn } = require('child_process')
    const sp = spawn(
    process.execPath,
    [
    '-e',
    `console.error('err output'); process.stdin.pipe(process.stdout)`
    ],
    // now sp.stderr will be null as well
    { stdio: ['pipe', 'inherit', process.stdout] }
    )

    sp.stdin.write('this input will become output\n')
    sp.stdin.end()
\end{lstlisting}

To send input to a child process created with spawn or exec we can call the write method of the return ChildProcess instance ( sp.stdin.write('...\\n')).
For the spawnSync and execSync functions an input option be used to achieve the same:

\begin{lstlisting}[style=ES6]
    'use strict'
    const { spawnSync } = require('child_process')

    spawnSync(
    process.execPath,
    [
    '-e',
    `console.error('err output'); process.stdin.pipe(process.stdout)`
    ],
    {
        input: 'this input will become output\n',
        stdio: ['pipe', 'inherit', 'ignore']
    }
    )
\end{lstlisting}

For the input option to work for spawnSync and execSync the stdio[0] option has to be pipe, otherwise the input option is ignored.

\section{ Writing Unit Tests }

\subsection{Assertions}

 The core assert module exports a function that will throw an AssertionError when the value passed to it is falsy (meaning that the value can be coerced to false with !!val):

 The core assert module has the following assertion methods:

\begin{lstlisting}[style=ES6]
    assert.ok(val) - the same as assert(val)

    assert.equal(val1, val2) - val1 == val2
    assert.notEqual(val1, val2) -val1 != val2

    // also checks type
    assert.strictEqual(val1, val2) - val1 === val2
    assert.notStrictEqual(val1, val2) - val1 !== val2

    // all values in an object
    assert.deepEqual(obj1, obj2)
    assert.notDeepEqual(obj1, obj2)

    // al values in an object, also hecking type
    assert.deepStrictEqual(obj1, obj2)
    assert.notDeepStrictEqual(obj1, obj2)

    assert.throws(function)
    assert.doesNotThrow(function)

    assert.rejects(promise||async function) -assert promise rjects
    assert.doesNotReject(promise||async function) -assert promise resolves

    assert.ifError(err) - check that an error object is falsy

    assert.match(string, regex)
    assert.doesNotMatch(string, regex)

    assert.fail() -force an AssertionError to be thrown
\end{lstlisting}

The assert module also exposes a strict object where namespaces for non-strict methods are strict, so the above code could also be written as:

\begin{lstlisting}[style=ES6]
    const assert = require('assert')
    const add = require('./get-add-from-somewhere.js')
    assert.strict.equal(add(2, 2), 4)
\end{lstlisting}

It's worth noting that assert.equal and other non-strict (i.e. coercive) assertion methods are deprecated, which means they may one day be removed from Node core. Therefore if using the Node core assert module, best practice would be always to use assert.strict rather than assert, or at least always use the strict methods (e.g. assert.strictEqual).

The error handling assertions (throws, ifError, rejects) are useful for asserting that error situations occur for synchronous, callback-based and promise-based APIs.

Let's start with an error case from an API that is synchronous:

\begin{lstlisting}[style=ES6]
    const assert = require('assert')
    const add = (a, b) => {
        if (typeof a !== 'number' || typeof b !== 'number') {
            throw Error('inputs must be numbers')
        }
        return a + b
    }
    // need to pass a function
    assert.throws(() => add('5', '5'), Error('inputs must be numbers'))
    assert.doesNotThrow(() => add(5, 5))
\end{lstlisting}

Notice that the invocation of add is wrapped inside another function. This is because the assert.throws and assert.doesNotThrow methods have to be , which they can then wrap and call to see if a throw occurs or not. When executed the above code will pass, which is to say, no output will occur and the process will exit.



\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\begin{lstlisting}[style=ES6]

\end{lstlisting}

\section{Appendix}

\subsection{JavaScript Function Definitions}

From:  \url{https://dmitripavlutin.com/6-ways-to-declare-javascript-functions/}

\subsubsection{Function declaration}

A function \textit{declaration} starts with \lstinline|function|.

An important property of this syntax is its \textit{hoisting} mechanism. Hoisting allows the function to be used before its declaration.

\begin{lstlisting}[style=ES6]
    function isNil(value) {
        return value == null;
    }
\end{lstlisting}

\subsubsection{Function expression (unnamed)}

An unnamed \textit{expression} containing the word \textit{function}.

\begin{lstlisting}[style=ES6]

    // create a method on an object
    { sum: function() {...},
      //
    }

    // use as a callback
    array.map(function(...) {...})

\end{lstlisting}


\subsubsection{Function expression (named)}

A named \textit{expression} containing the word \textit{function}.

\begin{lstlisting}[style=ES6]

    const isTruthy = function(value) {
        return !!value;
    }
    // or
    let count = function(...) {...}

\end{lstlisting}

\subsubsection{Shorthand method definition}

Like a function definition/expression, but missing the word \textit{function}.

\begin{lstlisting}[style=ES6]

    const collection = {
        items: [],
        add(...items) {
            this.items.push(...items);
        },
        get(index) {
            return this.items[index];
        }
    };

    collection.add('C', 'Java', 'PHP');

\end{lstlisting}

\subsubsection{Arrow function}

The arrow function does not itself create its execution context, but takes it lexically (contrary to function expression or function declaration, which create their own \lstinline|this| depending on invocation).

\begin{lstlisting}[style=ES6]

    const absValue = (number) => {
        if (number < 0) {
            return -number;
        }
        return number;
    }

\end{lstlisting}

\subsubsection{Generator function}

The generator function in JavaScript returns a Generator object.
Its syntax is similar to function expression, function declaration, or method declaration, but it requires a star character *.

\begin{lstlisting}[style=ES6]

    const obj = {
        *indexGenerator() {
            var index = 0;
            while(true) {
                yield index++;
            }
        }
    }

\end{lstlisting}


\begin{lstlisting}[style=ES6]
    he events module exports an EventEmitter constructor:

    const { EventEmitter } = require('events')

    In modern node the events module is the EventEmitter constructor as well:

    const EventEmitter = require('events')

    Both forms are fine for contemporary Node.js usage.

    To create a new event emitter, call the constructor with new:

    const myEmitter = new EventEmitter()

    A more typical pattern of usage with EventEmitter, however, is to inherit from it:

    class MyEmitter extends EventEmitter {
        constructor (opts = {}) {
            super(opts)
            this.name = opts.name
        }
    }

\end{lstlisting}





\end{document}




























